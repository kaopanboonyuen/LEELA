# ğŸ§  LEELA

**LE**v**E**raging **LA**rge Language Models and Neural Approximations
for Fault Prediction in Colored Petri Net Models

<!-- <p align="center">
  <img src="assets/leela_overview.png" width="720"/>
</p>

<p align="center">
  <b>A Neural-Symbolic Framework for Scalable Temporal Reasoning in Formal Verification</b>
</p> -->

---

## ğŸ“Œ Overview

**LEELA** is a **hybrid neuralâ€“symbolic verification framework** that bridges **formal model checking** and **modern AI reasoning**.
It is designed to **predict faults in reactive systems modeled by Colored Petri Nets (CPNs)** while mitigating the classical **state-space explosion problem**.

Unlike traditional model checkingâ€”which relies on exhaustive state explorationâ€”LEELA **learns temporal behavior patterns** from execution traces and **approximates Linear Temporal Logic (LTL) semantics** using a combination of:

* **GRU-based temporal modeling**
* **Self-attention for interpretability**
* **Pretrained Large Language Models (LLMs)** for semantic generalization

This enables **scalable, interpretable, and incremental fault prediction**, even under **partial or evolving specifications**.

---

## âœ¨ Key Contributions

* ğŸ”— **Neuralâ€“Symbolic Integration**
  Combines GRUs, attention mechanisms, and LLM embeddings to approximate LTL semantics.

* ğŸš€ **Scalable Alternative to Model Checking**
  Reduces reliance on exhaustive state-space traversal while preserving temporal reasoning power.

* ğŸ” **Interpretability by Design**
  Attention weights expose *which system states contribute to fault predictions*.

* ğŸ” **Incremental Verification Ready**
  Robust to model evolutionâ€”ideal for continuous integration and agile software development.

* ğŸ§  **LLM-Augmented Reasoning**
  Uses pretrained language models to encode temporal logic templates and semantic priors.

---

## ğŸ—ï¸ Architecture

<!-- <p align="center">
  <img src="assets/leela_architecture.png" width="760"/>
</p> -->

**LEELA Inference Pipeline**

1. **CPN Execution Traces** generated via *CPN Tools*
2. **State Encoding** into vector representations
3. **ENGRU (Enhanced GRU)** for temporal modeling
4. **LTL Embedding** via pretrained LLMs
5. **Cross-Attention Alignment** between traces and LTL semantics
6. **Fault Likelihood Prediction**

---

## ğŸ§® Formal Intuition

Given:

* A CPN state-space trace
  [
  \mathcal{T} = [s_1, s_2, \dots, s_T]
  ]
* An LTL specification template (\varphi)

LEELA computes:

* Temporal hidden states via GRUs
* Attention scores aligned with LTL semantic embeddings
* A **fault likelihood score** (\hat{y} \in [0,1])

This allows LEELA to **softly approximate temporal logic satisfaction** rather than relying on binary model checking outcomes.

---

## ğŸ“Š Experimental Results

### PETRINET_KKU Dataset

| Method               | Accuracy (%) | Precision (%) | Recall (%) | F1 (%)   |
| -------------------- | ------------ | ------------- | ---------- | -------- |
| State-space analysis | 81.2         | 78.5          | 76.9       | 77.7     |
| GRU-only             | 85.6         | 84.1          | 83.7       | 83.9     |
| GRU + Attention      | 88.9         | 87.5          | 88.1       | 87.8     |
| LLM Prompting        | 86.3         | 85.9          | 84.6       | 85.2     |
| **LEELA (Ours)**     | **92.4**     | **91.7**      | **92.1**   | **91.9** |

---

### Ablation Study

| Configuration         | Accuracy (%) | F1 (%)   | Fault Miss Rate (%) |
| --------------------- | ------------ | -------- | ------------------- |
| w/o Temporal Encoding | 84.7         | 83.9     | 12.6                |
| w/o Attention         | 87.1         | 86.5     | 10.3                |
| w/o LLM Knowledge     | 89.2         | 88.7     | 8.9                 |
| **Full LEELA**        | **92.4**     | **91.9** | **5.8**             |

â¡ï¸ **LLM integration and attention are critical for generalization and fault recall.**

---

## ğŸ“‚ Repository Structure

```
LEELA/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ PETRINET_KKU/
â”‚   â””â”€â”€ traces/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ engru.py
â”‚   â”œâ”€â”€ attention.py
â”‚   â””â”€â”€ leela.py
â”œâ”€â”€ ltl/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ llm_embeddings.py
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ ablation.py
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ leela_architecture.png
â”‚   â””â”€â”€ leela_overview.png
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/kaopanboonyuen/LEELA.git
cd LEELA
pip install -r requirements.txt
```

> **Dependencies**: PyTorch, NumPy, scikit-learn, HuggingFace Transformers

---

## ğŸš€ Quick Start

```bash
python experiments/train.py \
  --dataset PETRINET_KKU \
  --ltl_template G_F_safety \
  --use_llm true
```

To evaluate:

```bash
python experiments/evaluate.py --checkpoint checkpoints/leela.pt
```

---

## ğŸ”¬ Reproducibility

* All experiments are **fully deterministic**
* Random seeds are fixed
* Exact dataset splits are provided
* Matches results reported in the paper

---

## ğŸŒ Research Vision

LEELA is a step toward:

* ğŸ¤– **Agentic AIâ€“based Model Checking**
* ğŸ§  **Neural-Symbolic Formal Verification**
* ğŸ” **Continuous, Incremental System Assurance**

We believe future verification systems will *reason*, not just *explore*.

---